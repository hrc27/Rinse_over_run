{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNNConfig(object):\n",
    "    def __init__(\n",
    "        cell_type='lstm', window=20, forget_bias=1.0, \n",
    "        n_hidden_cells=(100), keep_prob=1.0, batch_size=64, epoch_num=100,\n",
    "        learning_rate=0.01, max_grad_norm=1.0, init_scale=0.1,\n",
    "    ):\n",
    "        self.cell_type = cell_type\n",
    "        self.window = window\n",
    "        self.forget_bias = forget_bias\n",
    "        self.n_hidden_cells = n_hidden_cells\n",
    "        self.keep_prob = keep_prob\n",
    "        self.batch_size = batch_size\n",
    "        self.epoch_num = epoch_num\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.init_scale = init_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\n",
    "    \"../data/raw/train_values.csv\", \n",
    "    index_col='row_id', \n",
    "    parse_dates=['timestamp'], \n",
    "    nrows=100000  \n",
    "    # We'll just load the first 100K so we know\n",
    "    # how to load the data using code\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['process_id', 'object_id', 'phase', 'timestamp', 'pipeline',\n",
       "       'supply_flow', 'supply_pressure', 'return_temperature',\n",
       "       'return_conductivity', 'return_turbidity', 'return_flow', 'supply_pump',\n",
       "       'supply_pre_rinse', 'supply_caustic', 'return_caustic', 'supply_acid',\n",
       "       'return_acid', 'supply_clean_water', 'return_recovery_water',\n",
       "       'return_drain', 'object_low_level', 'tank_level_pre_rinse',\n",
       "       'tank_level_caustic', 'tank_level_acid', 'tank_level_clean_water',\n",
       "       'tank_temperature_pre_rinse', 'tank_temperature_caustic',\n",
       "       'tank_temperature_acid', 'tank_concentration_caustic',\n",
       "       'tank_concentration_acid', 'tank_lsh_caustic', 'tank_lsh_acid',\n",
       "       'tank_lsh_clean_water', 'tank_lsh_pre_rinse', 'target_time_period'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "\n",
    "## Encoding data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's figure out how to preprocess the data into RNN-feedable form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's separate out `process_id`. We don't want to encode or scale this information. We'll just reattach after encoding and scaling the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_id_array = data.process_id.values\n",
    "data.drop(['process_id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `phase` will NOT be one-hot-encoded since there is an order to it, so we can simply encode it using 1, 2, .., 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase_categorical = CategoricalDtype(\n",
    "    categories=['pre_rinse', 'caustic', 'intermediate_rinse', \n",
    "                'acid', 'final_rinse'],\n",
    "    ordered=True\n",
    ")\n",
    "data.phase = data.phase.astype(phase_categorical)\n",
    "\n",
    "data.phase, phase_mapping_idx = data.phase.factorize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['pre_rinse', 'caustic', 'intermediate_rinse', 'acid', 'final_rinse'], dtype='object')"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phase_mapping_idx.categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ensure each sequence is in the right order using the `timestamp` column, but once sequences are set up, discard the column.\n",
    "  * The column is only useful insofar as it tells us which data point comes before or after others.\n",
    "  * But then again, **perhaps there is some signal from absoluate passage of time, so we can consider encoding this into UNIX times.** \n",
    " Something to try out.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.timestamp = data.timestamp.view(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ensure each sequence is from single process using `process_id`, but don't include it in data since it really is just an ID column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* One-hot encode `object_id` and `pipeline`.\n",
    "  * Although `object_id` is an ID column, there aren't that many objects (~100? **NOT SURE**) and there are multiple processes over each object. So `object_id` may carry valuable information.\n",
    "  * I'm not sure what `pipeline` is, but we'll treat it as a categorical column\n",
    "  * Below we first encode categorical columns into integers (and save that mapping in DataframeLabelEncoder.feature_encoder dictionary) and one-hot-encode them. (sklearn's OneHotEncoder requires that input is alreay integer-encoded.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['object_id', 'pipeline',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "class DataframeLabelEncoder(object):\n",
    "    def __init__(self, categorical_features):\n",
    "        assert isinstance(categorical_features, (list, np.ndarray))\n",
    "        self.categorical_features = categorical_features\n",
    "        self.feature_encoder = dict()\n",
    "        self.cat_feature_mask = None\n",
    "        \n",
    "    def fit(self, dataframe):\n",
    "        assert isinstance(dataframe, pd.DataFrame)\n",
    "        \n",
    "        if self.cat_feature_mask is None:\n",
    "            self.cat_feature_mask = np.zeros(shape=dataframe.shape[1], dtype=bool)\n",
    "        \n",
    "        for i, feature in enumerate(categorical_features):\n",
    "            le = LabelEncoder()\n",
    "            le.fit(dataframe[feature])\n",
    "            self.feature_encoder[feature] = le\n",
    "            \n",
    "            index = np.where(dataframe.columns == feature)[0][0]\n",
    "            self.cat_feature_mask[index] = True\n",
    "        \n",
    "    def transform(self, dataframe):\n",
    "        assert dataframe.shape[1] == self.cat_feature_mask.shape[-1]\n",
    "        \n",
    "        array = dataframe.values.copy()\n",
    "        for feature in categorical_features:\n",
    "            encoded = self.feature_encoder[feature].transform(\n",
    "                dataframe[feature]\n",
    "            )\n",
    "            index = np.where(dataframe.columns == feature)[0][0]\n",
    "            array[:, index] = encoded\n",
    "            \n",
    "        return array\n",
    "    \n",
    "    def fit_transform(self, dataframe):\n",
    "        self.fit(dataframe)\n",
    "        return self.transform(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = DataframeLabelEncoder(categorical_features)\n",
    "label_encoder.fit(data)\n",
    "encoded_data = label_encoder.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 34)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create One-hot encoder\n",
    "ohe = OneHotEncoder(categorical_features=label_encoder.cat_feature_mask, sparse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data = ohe.fit_transform(encoded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 84)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is the expected shape: # non categorical features = 34-2 = 32, + # unique objects = 43, + # unique pipelines = 9. Thats 84."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling data\n",
    "\n",
    "From EDA, it looks like some are normally distributed, some exponential... we'll just standardize.\n",
    "\n",
    "I created the following thinking that we should only standardize columns that were originally numerical (and not boolean or categorical), but it's too much work.. I think just standardizing everything is fine!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumericColumnsStandardScaler(object):\n",
    "    def __init__(self, ohe_feature_indices):\n",
    "        assert isinstance(ohe_feature_indices, (list, np.ndarray))\n",
    "        self._ohe_feature_indices = ohe_feature_indices\n",
    "        self._scaler = StandardScaler()\n",
    "        self.numerical_mask = None\n",
    "        \n",
    "    def fit(self, data):\n",
    "        assert isinstance(data, np.array)\n",
    "        assert data.shape[1] == self.num_feature_mask.shape[-1]\n",
    "        \n",
    "        # we only want features that were numerical from the beginning\n",
    "        # So we create a mask that excludes all columns that are results of\n",
    "        # one-hot-encoding categorical columns.\n",
    "        mask = np.ones(data.shape[1], type=bool)\n",
    "        mask[:self._ohe_feature_indices[-1]] = 0\n",
    "        self.numerical_mask = mask\n",
    "        \n",
    "        self._scaler.fit(data[self.numerical_mask])\n",
    "        \n",
    "    def transform(self, data):\n",
    "        assert isinstance(data, np.array)\n",
    "        assert data.shape[1] == self.num_feature_mask.shape[-1]\n",
    "        scaled_num_data = self._scaler.transform(data[self.numerical_mask])\n",
    "        res = np.concatenate((data[~self.numerical_mask], scaled_num_data), axis=1)\n",
    "        print(res.shape)\n",
    "        return res\n",
    "    \n",
    "    def fit_transform(self, data):\n",
    "        self.fit(data)\n",
    "        return self.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv(\"../data/raw/train_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Pipeline(\n",
    "    steps=[\n",
    "        ('label_encoder', label_encoder),\n",
    "        ('one_hot_encoder', ohe),\n",
    "        ('scaler', StandardScaler().fit(encoded_data))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOW THAT's OUR PREPROCESSOR!!! HOORAY!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok let's just create the padded array now. (What Holden did!) \n",
    "\n",
    "From our EDA, we know the maximum length of a process is **15107**.\n",
    "We also know there are 34 columns (excluding process id), but that gets encoded to **84 columns.** Following Holdens' work, \n",
    "\n",
    "> (m, T, f) - where m is the number of unique processes, T is the number of time-sequences, and f is the number of features\n",
    "\n",
    "Our final array shall have shape $(82, 15107, 84)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(process_id_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that we've only loaded first 100K rows. Update this part if you're using your own sampled data or the whole dataset!! # of unique process ids, max lengt will be different.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000,)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(process_id_array == pid).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Item wrong length 100000 instead of 5021.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-147-c2b4686b9568>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_id_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mpid_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_id_array\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mpid\u001b[0m  \u001b[0;31m# mask for this process id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mturbidity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpid_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mpreprocessed_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpid_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/biscuit-test/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2680\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2681\u001b[0m             \u001b[0;31m# either boolean or fancy integer index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2682\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2683\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2684\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/biscuit-test/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_array\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2718\u001b[0m                 raise ValueError('Item wrong length %d instead of %d.' %\n\u001b[0;32m-> 2719\u001b[0;31m                                  (len(key), len(self.index)))\n\u001b[0m\u001b[1;32m   2720\u001b[0m             \u001b[0;31m# check_bool_indexer will throw exception if Series key cannot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2721\u001b[0m             \u001b[0;31m# be reindexed to match DataFrame rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Item wrong length 100000 instead of 5021."
     ]
    }
   ],
   "source": [
    "arr = np.zeros((82, 15107, 84))\n",
    "for i, pid in enumerate(np.unique(process_id_array)):\n",
    "    pid_mask = process_id_array == pid  # mask for this process id\n",
    "    turbidity = labels[pid_mask]\n",
    "    preprocessed_data = preprocessor.transform(data[pid_mask])\n",
    "    \n",
    "    nrows = pid_mask.sum()\n",
    "    arr[i, :nrows, :] = preprocessed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping data\n",
    "\n",
    "But first, we need to figure out how we're going to approach this problem.\n",
    "\n",
    "Key things to consider:\n",
    "\n",
    "* **Sequence length**: the processes have different lengths\n",
    "  * Not only that, the longest process has 15107 data points---very long!!\n",
    "* **Prediction output**: for each process (=sequence), predict **one** value.\n",
    "\n",
    "### Turning variable-length sequences into fixed-length?\n",
    "Say our lookback window size is $L$. We want to reshape data so that each process is expressed in $n$ sequences of length $L$ where $n= \\text{length of process} - L + 1$. \n",
    "\n",
    "At the end of each process, i.e. at $n$th sequence, the RNN's state must be refreshed.\n",
    "\n",
    "It is probably the best to create an iterator rather than making and actual array of all of this. Some iterator that managers each process and lookback length as well as signal to refresh RNN states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following suggestions from https://danijar.com/variable-sequence-lengths-in-tensorflow/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def length(sequence):\n",
    "  used = tf.sign(tf.reduce_max(tf.abs(sequence), 2))\n",
    "  length = tf.reduce_sum(used, 1)\n",
    "  length = tf.cast(length, tf.int32)\n",
    "  return length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iterator(dataX, dataY, batch_size, num_steps):\n",
    "    data_len = len(dataY)\n",
    "    batch_len = int(data_len / batch_size)\n",
    "\n",
    "    if batch_len == 0:\n",
    "        raise ValueError(\"epoch_size == 0, decrease batch_size or num_steps\")\n",
    "\n",
    "    for i in range(batch_len):\n",
    "        input_x = dataX[i * batch_size: (i + 1) * batch_size]\n",
    "        input_y = dataY[i * batch_size: (i + 1) * batch_size]\n",
    "\n",
    "        yield (input_x, input_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def many_to_one_model_reproduce(output_dim=1):\n",
    "    model = tf.keras.models.Sequential(\n",
    "        layers=[\n",
    "            tf.keras.layers.LSTM(\n",
    "                rnn_cell_hidden_dim, input_shape=[14, 17], return_sequences=True,\n",
    "                activation='softsign'),\n",
    "            tf.keras.layers.LSTM(\n",
    "                rnn_cell_hidden_dim, activation='softsign'),\n",
    "            tf.keras.layers.Dense(\n",
    "                output_dim,\n",
    "                activation='softmax' if output_dim == 2 else 'sigmoid'),\n",
    "        ]\n",
    "    )\n",
    "    loss = 'binary_crossentropy' if output_dim == 1 else 'categorical_crossentropy'\n",
    "    adam = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "    model.compile(\n",
    "        loss=loss, optimizer=adam,\n",
    "        metrics=['accuracy', precision, recall, f1_score]\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:biscuit-test]",
   "language": "python",
   "name": "conda-env-biscuit-test-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
