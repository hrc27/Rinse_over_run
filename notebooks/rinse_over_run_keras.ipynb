{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/suzinyou/anaconda2/envs/biscuit-test/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/suzinyou/anaconda2/envs/biscuit-test/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/suzinyou/anaconda2/envs/biscuit-test/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/suzinyou/anaconda2/envs/biscuit-test/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow import keras\n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.api.types import CategoricalDtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNNConfig(object):\n",
    "    def __init__(\n",
    "        cell_type='lstm', window=20, forget_bias=1.0, \n",
    "        n_hidden_cells=(100), keep_prob=1.0, batch_size=64, epoch_num=100,\n",
    "        learning_rate=0.01, max_grad_norm=1.0, init_scale=0.1,\n",
    "    ):\n",
    "        self.cell_type = cell_type\n",
    "        self.window = window\n",
    "        self.forget_bias = forget_bias\n",
    "        self.n_hidden_cells = n_hidden_cells\n",
    "        self.keep_prob = keep_prob\n",
    "        self.batch_size = batch_size\n",
    "        self.epoch_num = epoch_num\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.init_scale = init_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\n",
    "    \"data/raw/train_values.csv\", \n",
    "    index_col='row_id', \n",
    "    parse_dates=['timestamp'], \n",
    "    nrows=100000  \n",
    "    # We'll just load the first 100K so we know\n",
    "    # how to load the data using code\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "\n",
    "## Encoding data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's figure out how to preprocess the data into RNN-feedable form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `phase` will NOT be one-hot-encoded since there is an order to it, so we can simply encode it using 1, 2, .., 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase_categorical = CategoricalDtype(\n",
    "    categories=['pre_rinse', 'caustic', 'intermediate_rinse', \n",
    "                'acid', 'final_rinse'],\n",
    "    ordered=True\n",
    ")\n",
    "data.phase = data.phase.astype(phase_categorical)\n",
    "\n",
    "data.phase, phase_mapping_idx = data.phase.factorize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ensure each sequence is in the right order using the `timestamp` column, but once sequences are set up, discard the column.\n",
    "  * The column is only useful insofar as it tells us which data point comes before or after others.\n",
    "  * But then again, perhaps there is some signal from absoluate passage of time, so we can consider encoding this into UNIX times. Something to try out.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.timestamp = data.timestamp.view(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ensure each sequence is from single process using `process_id`, but don't include it in data since it really is just an ID column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* One-hot encode `object_id` and `pipeline`.\n",
    "  * Although `object_id` is an ID column, there aren't that many objects (~100? **NOT SURE**) and there are multiple processes over each object. So `object_id` may carry valuable information.\n",
    "  * I'm not sure what `pipeline` is, but we'll treat it as a categorical column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['object_id', 'pipeline',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode to integer first, and also get mask of categorical features\n",
    "cat_feature_mask = np.zeros(shape=data.shape[1], dtype=int)\n",
    "feature_categories = dict()\n",
    "for i, col in enumerate(data.columns):\n",
    "    if col in categorical_features:\n",
    "        # encode to integer\n",
    "        categories = np.sort(data[col].unique())\n",
    "        data[col] = categories.searchsorted(data[col])\n",
    "        feature_categories[col] = categories\n",
    "        \n",
    "        # update mask\n",
    "        cat_feature_mask[i] = 1\n",
    "    \n",
    "# Create One-hot encoder\n",
    "ohe = OneHotEncoder(categorical_features=cat_feature_mask)\n",
    "data = ohe.fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we've only fitted the one-hot encoder. Using `transform` returns a numpy output. Will deal with this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping data\n",
    "\n",
    "Say our lookback window size is $L$. We want to reshape data so that each process is expressed in $n$ sequences of length $L$ where $n= \\text{length of process} - L + 1$. \n",
    "\n",
    "At the end of each process, i.e. at $n$th sequence, the RNN's state must be refreshed.\n",
    "\n",
    "It is probably the best to create an iterator rather than making and actual array of all of this. Some iterator that managers each process and lookback length as well as signal to refresh RNN states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iterator(dataX, dataY, batch_size, num_steps):\n",
    "    data_len = len(dataY)\n",
    "    batch_len = int(data_len / batch_size)\n",
    "\n",
    "    if batch_len == 0:\n",
    "        raise ValueError(\"epoch_size == 0, decrease batch_size or num_steps\")\n",
    "\n",
    "    for i in range(batch_len):\n",
    "        input_x = dataX[i * batch_size: (i + 1) * batch_size]\n",
    "        input_y = dataY[i * batch_size: (i + 1) * batch_size]\n",
    "\n",
    "        yield (input_x, input_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def many_to_one_model_reproduce(output_dim=1):\n",
    "    model = tf.keras.models.Sequential(\n",
    "        layers=[\n",
    "            tf.keras.layers.LSTM(\n",
    "                rnn_cell_hidden_dim, input_shape=[14, 17], return_sequences=True,\n",
    "                activation='softsign'),\n",
    "            tf.keras.layers.LSTM(\n",
    "                rnn_cell_hidden_dim, activation='softsign'),\n",
    "            tf.keras.layers.Dense(\n",
    "                output_dim,\n",
    "                activation='softmax' if output_dim == 2 else 'sigmoid'),\n",
    "        ]\n",
    "    )\n",
    "    loss = 'binary_crossentropy' if output_dim == 1 else 'categorical_crossentropy'\n",
    "    adam = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "    model.compile(\n",
    "        loss=loss, optimizer=adam,\n",
    "        metrics=['accuracy', precision, recall, f1_score]\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:biscuit-test]",
   "language": "python",
   "name": "conda-env-biscuit-test-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
