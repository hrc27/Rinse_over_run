{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/suzinyou/anaconda2/envs/biscuit-test/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/suzinyou/anaconda2/envs/biscuit-test/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/suzinyou/anaconda2/envs/biscuit-test/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/suzinyou/anaconda2/envs/biscuit-test/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/suzinyou/anaconda2/envs/biscuit-test/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/suzinyou/anaconda2/envs/biscuit-test/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNNConfig(object):\n",
    "    def __init__(\n",
    "        cell_type='lstm', window=20, forget_bias=1.0, \n",
    "        n_hidden_cells=(100), keep_prob=1.0, batch_size=64, epoch_num=100,\n",
    "        learning_rate=0.01, max_grad_norm=1.0, init_scale=0.1,\n",
    "    ):\n",
    "        self.cell_type = cell_type\n",
    "        self.window = window\n",
    "        self.forget_bias = forget_bias\n",
    "        self.n_hidden_cells = n_hidden_cells\n",
    "        self.keep_prob = keep_prob\n",
    "        self.batch_size = batch_size\n",
    "        self.epoch_num = epoch_num\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.init_scale = init_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/suzinyou/anaconda2/envs/biscuit-test/lib/python3.5/site-packages/numpy/lib/arraysetops.py:518: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\n",
    "    \"../data/raw/train_values.csv\", \n",
    "    index_col='row_id', \n",
    "    parse_dates=['timestamp'], \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "1. Exclude `final_rinse` data\n",
    "2. Sample according to the phase distributions\n",
    "3. Separate out `process_id`. We don't want to encode or scale this information. We'll just reattach after encoding and scaling the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data.phase != 'final_rinse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pids = data.process_id.unique()\n",
    "phase_categorical = CategoricalDtype(\n",
    "    categories=['pre_rinse', 'caustic', 'intermediate_rinse', 'acid', 'final_rinse'],\n",
    "    ordered=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_idx = np.random.permutation(np.arange(len(pids)))\n",
    "n = len(shuffled_idx)\n",
    "\n",
    "phase_pid_idx = {\n",
    "    'pre_rinse': shuffled_idx[:int(n * 0.1)],\n",
    "    'caustic': shuffled_idx[int(n * 0.1): int(n * 0.4)],\n",
    "    'intermediate_rinse': shuffled_idx[int(n * 0.4): int(n * 0.7)],\n",
    "    'acid': shuffled_idx[int(n * 0.7):]\n",
    "}\n",
    "# pid: phase_cutoff\n",
    "# pid = pids[index]\n",
    "pid2phase_list = []\n",
    "for phase in phase_pid_idx:\n",
    "    cur_pids = pids[phase_pid_idx[phase]]\n",
    "    pid2phase_list.extend(list(zip(cur_pids, np.repeat(phase, len(cur_pids)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "pid2phase_cutoff = dict(pid2phase_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5005"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['process_id', 'object_id', 'phase', 'timestamp', 'pipeline',\n",
       "       'supply_flow', 'supply_pressure', 'return_temperature',\n",
       "       'return_conductivity', 'return_turbidity', 'return_flow', 'supply_pump',\n",
       "       'supply_pre_rinse', 'supply_caustic', 'return_caustic', 'supply_acid',\n",
       "       'return_acid', 'supply_clean_water', 'return_recovery_water',\n",
       "       'return_drain', 'object_low_level', 'tank_level_pre_rinse',\n",
       "       'tank_level_caustic', 'tank_level_acid', 'tank_level_clean_water',\n",
       "       'tank_temperature_pre_rinse', 'tank_temperature_caustic',\n",
       "       'tank_temperature_acid', 'tank_concentration_caustic',\n",
       "       'tank_concentration_acid', 'tank_lsh_caustic', 'tank_lsh_acid',\n",
       "       'tank_lsh_clean_water', 'tank_lsh_pre_rinse', 'target_time_period'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's separate out `process_id`. We don't want to encode or scale this information. We'll just reattach after encoding and scaling the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_id_array = data.process_id.values\n",
    "data.drop(['process_id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's figure out how to preprocess the data into RNN-feedable form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `phase` will NOT be one-hot-encoded since there is an order to it, so we can simply encode it using 1, 2, .., 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase_categorical = CategoricalDtype(\n",
    "    categories=['pre_rinse', 'caustic', 'intermediate_rinse', 'acid', 'final_rinse'],\n",
    "    ordered=True\n",
    ")\n",
    "data.phase = data.phase.astype(phase_categorical)\n",
    "\n",
    "data.phase, phase_mapping_idx = data.phase.factorize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['pre_rinse', 'caustic', 'intermediate_rinse', 'acid', 'final_rinse'], dtype='object')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phase_mapping_idx.categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ensure each sequence is in the right order using the `timestamp` column, but once sequences are set up, discard the column.\n",
    "  * The column is only useful insofar as it tells us which data point comes before or after others.\n",
    "  * But then again, **perhaps there is some signal from absoluate passage of time, so we can consider encoding this into UNIX times.** \n",
    " Something to try out.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.timestamp = data.timestamp.view(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ensure each sequence is from single process using `process_id`, but don't include it in data since it really is just an ID column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* One-hot encode `object_id` and `pipeline`.\n",
    "  * Although `object_id` is an ID column, there aren't that many objects (~100? **NOT SURE**) and there are multiple processes over each object. So `object_id` may carry valuable information.\n",
    "  * I'm not sure what `pipeline` is, but we'll treat it as a categorical column\n",
    "  * Below we first encode categorical columns into integers (and save that mapping in DataframeLabelEncoder.feature_encoder dictionary) and one-hot-encode them. (sklearn's OneHotEncoder requires that input is alreay integer-encoded.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['object_id', 'pipeline',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "class DataframeLabelEncoder(object):\n",
    "    def __init__(self, categorical_features):\n",
    "        assert isinstance(categorical_features, (list, np.ndarray))\n",
    "        self.categorical_features = categorical_features\n",
    "        self.feature_encoder = dict()\n",
    "        self.cat_feature_mask = None\n",
    "        \n",
    "    def fit(self, dataframe):\n",
    "        assert isinstance(dataframe, pd.DataFrame)\n",
    "        \n",
    "        if self.cat_feature_mask is None:\n",
    "            self.cat_feature_mask = np.zeros(shape=dataframe.shape[1], dtype=bool)\n",
    "        \n",
    "        for i, feature in enumerate(categorical_features):\n",
    "            le = LabelEncoder()\n",
    "            le.fit(dataframe[feature])\n",
    "            self.feature_encoder[feature] = le\n",
    "            \n",
    "            index = np.where(dataframe.columns == feature)[0][0]\n",
    "            self.cat_feature_mask[index] = True\n",
    "        \n",
    "    def transform(self, dataframe):\n",
    "        assert dataframe.shape[1] == self.cat_feature_mask.shape[-1]\n",
    "        \n",
    "        array = dataframe.values.copy()\n",
    "        for feature in categorical_features:\n",
    "            encoded = self.feature_encoder[feature].transform(\n",
    "                dataframe[feature]\n",
    "            )\n",
    "            index = np.where(dataframe.columns == feature)[0][0]\n",
    "            array[:, index] = encoded\n",
    "            \n",
    "        return array\n",
    "    \n",
    "    def fit_transform(self, dataframe):\n",
    "        self.fit(dataframe)\n",
    "        return self.transform(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = DataframeLabelEncoder(categorical_features)\n",
    "label_encoder.fit(data)\n",
    "encoded_data = label_encoder.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4475493, 34)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create One-hot encoder\n",
    "ohe = OneHotEncoder(categorical_features=label_encoder.cat_feature_mask, sparse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data = ohe.fit_transform(encoded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4475493, 137)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.pipeline.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.object_id.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is the expected shape: # non categorical features = 34-2 = 32, + # unique objects = 94, + # unique pipelines = 11. Thats 137."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling data\n",
    "\n",
    "From EDA, it looks like some are normally distributed, some exponential... we'll just standardize.\n",
    "\n",
    "I created the following thinking that we should only standardize columns that were originally numerical (and not boolean or categorical), but it's too much work.. I think just standardizing everything is fine!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumericColumnsStandardScaler(object):\n",
    "    def __init__(self, ohe_feature_indices):\n",
    "        assert isinstance(ohe_feature_indices, (list, np.ndarray))\n",
    "        self._ohe_feature_indices = ohe_feature_indices\n",
    "        self._scaler = StandardScaler()\n",
    "        self.numerical_mask = None\n",
    "        \n",
    "    def fit(self, data):\n",
    "        assert isinstance(data, np.array)\n",
    "        assert data.shape[1] == self.num_feature_mask.shape[-1]\n",
    "        \n",
    "        # we only want features that were numerical from the beginning\n",
    "        # So we create a mask that excludes all columns that are results of\n",
    "        # one-hot-encoding categorical columns.\n",
    "        mask = np.ones(data.shape[1], type=bool)\n",
    "        mask[:self._ohe_feature_indices[-1]] = 0\n",
    "        self.numerical_mask = mask\n",
    "        \n",
    "        self._scaler.fit(data[self.numerical_mask])\n",
    "        \n",
    "    def transform(self, data):\n",
    "        assert isinstance(data, np.array)\n",
    "        assert data.shape[1] == self.num_feature_mask.shape[-1]\n",
    "        scaled_num_data = self._scaler.transform(data[self.numerical_mask])\n",
    "        res = np.concatenate((data[~self.numerical_mask], scaled_num_data), axis=1)\n",
    "        print(res.shape)\n",
    "        return res\n",
    "    \n",
    "    def fit_transform(self, data):\n",
    "        self.fit(data)\n",
    "        return self.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv(\"../data/raw/train_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Pipeline(\n",
    "    steps=[\n",
    "        ('label_encoder', label_encoder),\n",
    "        ('one_hot_encoder', ohe),\n",
    "        ('scaler', StandardScaler().fit(encoded_data))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOW THAT's OUR PREPROCESSOR!!! HOORAY!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok let's just create the padded array now. (What Holden did!) \n",
    "\n",
    "From our EDA, we know the maximum length of a process is **15107**.\n",
    "We also know there are 34 columns (excluding process id), but that gets encoded to **84 columns.** Following Holdens' work, \n",
    "\n",
    "> (m, T, f) - where m is the number of unique processes, T is the number of time-sequences, and f is the number of features\n",
    "\n",
    "Our final array shall have shape $(82, 15107, 84)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that we've only loaded first 100K rows. Update this part if you're using your own sampled data or the whole dataset!! # of unique process ids, max lengt will be different.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000,)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(process_id_array == pid).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we transform our data, let's drop unnecessary ones according to our `phase_pid_idx` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4475493, 137)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5005"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.phase.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phase_idx(phase):\n",
    "    phase_index = np.where(\n",
    "        phase_categorical.categories == phase\n",
    "    )[0][0]\n",
    "    return phase_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "arr = np.zeros((5005, 15107, 137))\n",
    "for i, pid in enumerate(pids):\n",
    "    pid_mask = process_id_array == pid  # mask for this process id\n",
    "    \n",
    "    # Cutoff up to appropriate phase\n",
    "    phase_idx = get_phase_idx(pid2phase_cutoff[pid])\n",
    "    pid_data = data[pid_mask]\n",
    "    \n",
    "    truncated_data = pid_data[pid_data.phase <= phase_idx]\n",
    "    if truncated_data.shape[0] == 0:\n",
    "        continue\n",
    "    \n",
    "    preprocessed_data = preprocessor.transform(truncated_data)\n",
    "    \n",
    "    nrows = preprocessed_data.shape[0]\n",
    "    arr[i, :nrows, :] = preprocessed_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_process_mask = arr[:, 0].sum(axis=1) == 0  # To get rid of process ids for which the phase cutoff got rid of everythinG@@1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_process_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = arr[~empty_process_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trailing_zeros_mask = arr[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `arr` is our final data :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can ignore this part ------------- I was thinking........weeks ago... maybe helpful?? idk..\n",
    "## Reshaping data\n",
    "\n",
    "But first, we need to figure out how we're going to approach this problem.\n",
    "\n",
    "Key things to consider:\n",
    "\n",
    "* **Sequence length**: the processes have different lengths\n",
    "  * Not only that, the longest process has 15107 data points---very long!!\n",
    "* **Prediction output**: for each process (=sequence), predict **one** value.\n",
    "\n",
    "### Turning variable-length sequences into fixed-length?\n",
    "Say our lookback window size is $L$. We want to reshape data so that each process is expressed in $n$ sequences of length $L$ where $n= \\text{length of process} - L + 1$. \n",
    "\n",
    "At the end of each process, i.e. at $n$th sequence, the RNN's state must be refreshed.\n",
    "\n",
    "It is probably the best to create an iterator rather than making and actual array of all of this. Some iterator that managers each process and lookback length as well as signal to refresh RNN states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following suggestions from https://danijar.com/variable-sequence-lengths-in-tensorflow/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def length(sequence):\n",
    "  used = tf.sign(tf.reduce_max(tf.abs(sequence), 2))\n",
    "  length = tf.reduce_sum(used, 1)\n",
    "  length = tf.cast(length, tf.int32)\n",
    "  return length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iterator(dataX, dataY, batch_size, num_steps):\n",
    "    data_len = len(dataY)\n",
    "    batch_len = int(data_len / batch_size)\n",
    "\n",
    "    if batch_len == 0:\n",
    "        raise ValueError(\"epoch_size == 0, decrease batch_size or num_steps\")\n",
    "\n",
    "    for i in range(batch_len):\n",
    "        input_x = dataX[i * batch_size: (i + 1) * batch_size]\n",
    "        input_y = dataY[i * batch_size: (i + 1) * batch_size]\n",
    "\n",
    "        yield (input_x, input_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def many_to_one_model_reproduce(output_dim=1):\n",
    "    model = tf.keras.models.Sequential(\n",
    "        layers=[\n",
    "            tf.keras.layers.LSTM(\n",
    "                rnn_cell_hidden_dim, input_shape=[14, 17], return_sequences=True,\n",
    "                activation='softsign'),\n",
    "            tf.keras.layers.LSTM(\n",
    "                rnn_cell_hidden_dim, activation='softsign'),\n",
    "            tf.keras.layers.Dense(\n",
    "                output_dim,\n",
    "                activation='softmax' if output_dim == 2 else 'sigmoid'),\n",
    "        ]\n",
    "    )\n",
    "    loss = 'binary_crossentropy' if output_dim == 1 else 'categorical_crossentropy'\n",
    "    adam = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "    model.compile(\n",
    "        loss=loss, optimizer=adam,\n",
    "        metrics=['accuracy', precision, recall, f1_score]\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:biscuit-test]",
   "language": "python",
   "name": "conda-env-biscuit-test-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "486px",
    "left": "857.6px",
    "right": "20px",
    "top": "120px",
    "width": "344px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
