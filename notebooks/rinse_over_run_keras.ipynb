{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/suzinyou/anaconda2/envs/biscuit-test/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/suzinyou/anaconda2/envs/biscuit-test/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/suzinyou/anaconda2/envs/biscuit-test/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/suzinyou/anaconda2/envs/biscuit-test/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/suzinyou/anaconda2/envs/biscuit-test/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/suzinyou/anaconda2/envs/biscuit-test/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\n",
    "    \"../data/raw/train_values.csv\", \n",
    "    index_col='row_id', \n",
    "    parse_dates=['timestamp'], nrows=10 ** 5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "1. Exclude `final_rinse` data\n",
    "2. Sample according to the phase distributions\n",
    "3. Separate out `process_id`. We don't want to encode or scale this information. We'll just reattach after encoding and scaling the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data.phase != 'final_rinse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pids = data.process_id.unique()\n",
    "phase_categorical = CategoricalDtype(\n",
    "    categories=['pre_rinse', 'caustic', 'intermediate_rinse', 'acid', 'final_rinse'],\n",
    "    ordered=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_idx = np.random.permutation(np.arange(len(pids)))\n",
    "n = len(shuffled_idx)\n",
    "\n",
    "phase_pid_idx = {\n",
    "    'pre_rinse': shuffled_idx[:int(n * 0.1)],\n",
    "    'caustic': shuffled_idx[int(n * 0.1): int(n * 0.4)],\n",
    "    'intermediate_rinse': shuffled_idx[int(n * 0.4): int(n * 0.7)],\n",
    "    'acid': shuffled_idx[int(n * 0.7):]\n",
    "}\n",
    "# pid: phase_cutoff\n",
    "# pid = pids[index]\n",
    "pid2phase_list = []\n",
    "for phase in phase_pid_idx:\n",
    "    cur_pids = pids[phase_pid_idx[phase]]\n",
    "    pid2phase_list.extend(list(zip(cur_pids, np.repeat(phase, len(cur_pids)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pid2phase_cutoff = dict(pid2phase_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 0\n",
    "for pid in pids:\n",
    "    cur_length = data[data.process_id == pid].shape[0]\n",
    "    if cur_length > max_length:\n",
    "        max_length = cur_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2296"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['process_id', 'object_id', 'phase', 'timestamp', 'pipeline',\n",
       "       'supply_flow', 'supply_pressure', 'return_temperature',\n",
       "       'return_conductivity', 'return_turbidity', 'return_flow', 'supply_pump',\n",
       "       'supply_pre_rinse', 'supply_caustic', 'return_caustic', 'supply_acid',\n",
       "       'return_acid', 'supply_clean_water', 'return_recovery_water',\n",
       "       'return_drain', 'object_low_level', 'tank_level_pre_rinse',\n",
       "       'tank_level_caustic', 'tank_level_acid', 'tank_level_clean_water',\n",
       "       'tank_temperature_pre_rinse', 'tank_temperature_caustic',\n",
       "       'tank_temperature_acid', 'tank_concentration_caustic',\n",
       "       'tank_concentration_acid', 'tank_lsh_caustic', 'tank_lsh_acid',\n",
       "       'tank_lsh_clean_water', 'tank_lsh_pre_rinse', 'target_time_period'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's separate out `process_id`. We don't want to encode or scale this information. We'll just reattach after encoding and scaling the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_id_array = data.process_id.values\n",
    "data.drop(['process_id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's figure out how to preprocess the data into RNN-feedable form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `phase` will NOT be one-hot-encoded since there is an order to it, so we can simply encode it using 1, 2, .., 5. \n",
    "  * Using `data.phase.factorize()` renders the phases to be 0, 1, ... so let's add 1 to all after that line.\n",
    "  * We don't want processes that end in `pre_rinse` stage to be lost when masking 0s.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase_categorical = CategoricalDtype(\n",
    "    categories=['pre_rinse', 'caustic', 'intermediate_rinse', 'acid', 'final_rinse'],\n",
    "    ordered=True\n",
    ")\n",
    "data.phase = data.phase.astype(phase_categorical)\n",
    "\n",
    "data.phase, phase_mapping_idx = data.phase.factorize()\n",
    "data.phase = data.phase + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['pre_rinse', 'caustic', 'intermediate_rinse', 'acid', 'final_rinse'], dtype='object')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phase_mapping_idx.categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ensure each sequence is in the right order using the `timestamp` column, but once sequences are set up, discard the column.\n",
    "  * The column is only useful insofar as it tells us which data point comes before or after others.\n",
    "  * But then again, **perhaps there is some signal from absoluate passage of time, so we can consider encoding this into UNIX times.** \n",
    " Something to try out.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.timestamp = data.timestamp.view(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ensure each sequence is from single process using `process_id`, but don't include it in data since it really is just an ID column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* One-hot encode `object_id` and `pipeline`.\n",
    "  * Although `object_id` is an ID column, there aren't that many objects (~100? **NOT SURE**) and there are multiple processes over each object. So `object_id` may carry valuable information.\n",
    "  * I'm not sure what `pipeline` is, but we'll treat it as a categorical column\n",
    "  * Below we first encode categorical columns into integers (and save that mapping in DataframeLabelEncoder.feature_encoder dictionary) and one-hot-encode them. (sklearn's OneHotEncoder requires that input is alreay integer-encoded.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['object_id', 'pipeline',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "class DataframeLabelEncoder(object):\n",
    "    def __init__(self, categorical_features):\n",
    "        assert isinstance(categorical_features, (list, np.ndarray))\n",
    "        self.categorical_features = categorical_features\n",
    "        self.feature_encoder = dict()\n",
    "        self.cat_feature_mask = None\n",
    "        \n",
    "    def fit(self, dataframe):\n",
    "        assert isinstance(dataframe, pd.DataFrame)\n",
    "        \n",
    "        if self.cat_feature_mask is None:\n",
    "            self.cat_feature_mask = np.zeros(shape=dataframe.shape[1], dtype=bool)\n",
    "        \n",
    "        for i, feature in enumerate(categorical_features):\n",
    "            le = LabelEncoder()\n",
    "            le.fit(dataframe[feature])\n",
    "            self.feature_encoder[feature] = le\n",
    "            \n",
    "            index = np.where(dataframe.columns == feature)[0][0]\n",
    "            self.cat_feature_mask[index] = True\n",
    "        \n",
    "    def transform(self, dataframe):\n",
    "        assert dataframe.shape[1] == self.cat_feature_mask.shape[-1]\n",
    "        \n",
    "        array = dataframe.values.copy()\n",
    "        for feature in categorical_features:\n",
    "            encoded = self.feature_encoder[feature].transform(\n",
    "                dataframe[feature]\n",
    "            )\n",
    "            index = np.where(dataframe.columns == feature)[0][0]\n",
    "            array[:, index] = encoded\n",
    "            \n",
    "        return array\n",
    "    \n",
    "    def fit_transform(self, dataframe):\n",
    "        self.fit(dataframe)\n",
    "        return self.transform(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = DataframeLabelEncoder(categorical_features)\n",
    "label_encoder.fit(data)\n",
    "encoded_data = label_encoder.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75709, 34)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create One-hot encoder\n",
    "ohe = OneHotEncoder(categorical_features=label_encoder.cat_feature_mask, sparse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data = ohe.fit_transform(encoded_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that is the expected shape: # non categorical features (= #num features - # num categorical features) + # unique objects + # unique pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75709, 84)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.pipeline.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.object_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ENCODED_FEATURES = encoded_data.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling data\n",
    "\n",
    "From EDA, it looks like some are normally distributed, some exponential... we'll just standardize.\n",
    "\n",
    "I thought that we should only standardize columns that were originally numerical (and not boolean or categorical), but it's too much work.. I think just standardizing everything is fine!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv(\"../data/raw/train_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Pipeline(\n",
    "    steps=[\n",
    "        ('label_encoder', label_encoder),\n",
    "        ('one_hot_encoder', ohe),\n",
    "        ('scaler', StandardScaler().fit(encoded_data))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOW THAT's OUR PREPROCESSOR!!! HOORAY!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok let's just create the padded array now. (What Holden did!) \n",
    "\n",
    "From our EDA, we know the maximum length of a process is **15107**.\n",
    "We also know there are 34 columns (excluding process id), but that gets encoded to **84 columns.** Following Holdens' work, \n",
    "\n",
    "> (m, T, f) - where m is the number of unique processes, T is the number of time-sequences, and f is the number of features\n",
    "\n",
    "Our final array shall have shape $(82, 15107, 84)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that we've only loaded first 100K rows. Update this part if you're using your own sampled data or the whole dataset!! # of unique process ids, max lengt will be different.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000,)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(process_id_array == pid).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we transform our data, let's drop unnecessary ones according to our `phase_pid_idx` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4475493, 137)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5005"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's cut them off!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phase_idx(phase):\n",
    "    phase_index = np.where(\n",
    "        phase_categorical.categories == phase\n",
    "    )[0][0]\n",
    "    return phase_index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "arr = np.zeros((len(pids), max_length, 84))\n",
    "for i, pid in enumerate(pids):\n",
    "    pid_mask = process_id_array == pid  # mask for this process id\n",
    "    \n",
    "    # Cutoff up to appropriate phase\n",
    "    phase_idx = get_phase_idx(pid2phase_cutoff[pid])\n",
    "    pid_data = data[pid_mask]\n",
    "    \n",
    "    truncated_data = pid_data[pid_data.phase <= phase_idx]\n",
    "    if truncated_data.shape[0] == 0:\n",
    "        continue\n",
    "    \n",
    "    preprocessed_data = preprocessor.transform(truncated_data)\n",
    "    \n",
    "    nrows = preprocessed_data.shape[0]\n",
    "    arr[i, :nrows, :] = preprocessed_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_process_mask = arr[:, 0].sum(axis=1) == 0  # To get rid of process ids for which the phase cutoff got rid of everythinG@@1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = arr[~empty_process_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.float32(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `arr` is our final data :)\n",
    "\n",
    "## Prep labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((70, 2296, 84), (5021, 2))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_mask = np.zeros(labels.shape[0], dtype=bool)\n",
    "final_pids = pids[~empty_process_mask]\n",
    "for i, pid in enumerate(labels.process_id):\n",
    "    if pid in final_pids:\n",
    "        labels_mask[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = labels.final_rinse_total_turbidity_liter[labels_mask].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can ignore this part ------------- I was thinking........weeks ago... maybe helpful?? idk..\n",
    "## Reshaping data\n",
    "\n",
    "But first, we need to figure out how we're going to approach this problem.\n",
    "\n",
    "Key things to consider:\n",
    "\n",
    "* **Sequence length**: the processes have different lengths\n",
    "  * Not only that, the longest process has 15107 data points---very long!!\n",
    "* **Prediction output**: for each process (=sequence), predict **one** value.\n",
    "\n",
    "### Turning variable-length sequences into fixed-length?\n",
    "Say our lookback window size is $L$. We want to reshape data so that each process is expressed in $n$ sequences of length $L$ where $n= \\text{length of process} - L + 1$. \n",
    "\n",
    "At the end of each process, i.e. at $n$th sequence, the RNN's state must be refreshed.\n",
    "\n",
    "It is probably the best to create an iterator rather than making and actual array of all of this. Some iterator that managers each process and lookback length as well as signal to refresh RNN states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following suggestions from https://danijar.com/variable-sequence-lengths-in-tensorflow/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def length(sequence):\n",
    "  used = tf.sign(tf.reduce_max(tf.abs(sequence), 2))\n",
    "  length = tf.reduce_sum(used, 1)\n",
    "  length = tf.cast(length, tf.int32)\n",
    "  return length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iterator(dataX, dataY, batch_size, num_steps):\n",
    "    data_len = len(dataY)\n",
    "    batch_len = int(data_len / batch_size)\n",
    "\n",
    "    if batch_len == 0:\n",
    "        raise ValueError(\"epoch_size == 0, decrease batch_size or num_steps\")\n",
    "\n",
    "    for i in range(batch_len):\n",
    "        input_x = dataX[i * batch_size: (i + 1) * batch_size]\n",
    "        input_y = dataY[i * batch_size: (i + 1) * batch_size]\n",
    "\n",
    "        yield (input_x, input_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REAL DEAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First try with Keras.\n",
    "\n",
    "First set up config object. This is how we'll keep track of which hyperparms we uesed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNConfig(object):\n",
    "    def __init__(\n",
    "        self, cell_type='lstm', window=20, forget_bias=1.0, \n",
    "        n_hidden_cells=100, n_layers=1, keep_prob=1.0, batch_size=1, \n",
    "        epoch_num=100, learning_rate=0.01, max_grad_norm=1.0, init_scale=0.1,\n",
    "    ):\n",
    "        self.cell_type = cell_type\n",
    "        self.window = window\n",
    "        self.forget_bias = forget_bias\n",
    "        self.n_hidden_cells = n_hidden_cells\n",
    "        self.keep_prob = keep_prob\n",
    "        self.batch_size = batch_size\n",
    "        self.epoch_num = epoch_num\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.init_scale = init_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function that creates the model, according to our config input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def many_to_one_model(config):\n",
    "    assert isinstance(config, RNNConfig)\n",
    "    model = tf.keras.models.Sequential()\n",
    "    \n",
    "    # Masking layer\n",
    "    model.add(\n",
    "        tf.keras.layers.Masking(\n",
    "            mask_value=0., input_shape=(config.window, N_ENCODED_FEATURES)))\n",
    "    \n",
    "    if config.n_layers == 1:\n",
    "        model.add(\n",
    "            tf.keras.layers.LSTM(\n",
    "                config.n_hidden_cells, input_shape=[config.window, N_ENCODED_FEATURES],\n",
    "                activation='relu'),)\n",
    "    elif config.n_layers == 2:\n",
    "        model.add(\n",
    "            tf.keras.layers.LSTM(\n",
    "                config.n_hidden_cells, input_shape=[config.window, N_ENCODED_FEATURES], return_sequences=True,\n",
    "                activation='relu')\n",
    "        )\n",
    "        model.add(\n",
    "            tf.keras.layers.LSTM(\n",
    "               config.n_hidden_cells, activation='relu'),\n",
    "        )\n",
    "    else:\n",
    "        raise NotImplementedError(\"Keep n_layers <= 2.\")\n",
    "        \n",
    "    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    loss = 'mean_squared_error'\n",
    "    rmsprop = tf.keras.optimizers.RMSprop(lr=config.learning_rate)\n",
    "    model.compile(\n",
    "        loss=loss, optimizer=rmsprop,\n",
    "        metrics=[tf.keras.metrics.mean_squared_error]\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Sanity check that model works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_config = RNNConfig(\n",
    "    window=max_length, \n",
    "    n_hidden_cells=50, \n",
    "    n_layers=1, \n",
    "    batch_size=1, \n",
    "    epoch_num=2\n",
    ")\n",
    "\n",
    "model = many_to_one_model(basic_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 56 samples, validate on 14 samples\n",
      "Epoch 1/2\n",
      "56/56 [==============================] - 149s 3s/step - loss: 4822549169414.8564 - mean_squared_error: 4822549169414.8564 - val_loss: 81770475728566.8594 - val_mean_squared_error: 81770475728566.8594\n",
      "Epoch 2/2\n",
      "56/56 [==============================] - 139s 2s/step - loss: 4822548847537.1436 - mean_squared_error: 4822548847537.1436 - val_loss: 81770475728566.8594 - val_mean_squared_error: 81770475728566.8594\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x128bbf828>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=arr, y=y, batch_size=basic_config.batch_size, epochs=basic_config.epoch_num, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Now some more complex model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_1 = RNNConfig(\n",
    "    window=max_length, \n",
    "    n_hidden_cells=100, \n",
    "    n_layers=2, \n",
    "    batch_size=1, \n",
    "    epoch_num=100\n",
    ")\n",
    "\n",
    "model_1 = many_to_one_model(config_1)\n",
    "model.fit(\n",
    "    x=arr, y=y, \n",
    "    batch_size=config_1.batch_size, \n",
    "    epochs=config_1.epoch_num, \n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt at inspecting test data... for the first time...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/suzinyou/anaconda2/envs/biscuit-test/lib/python3.5/site-packages/numpy/lib/arraysetops.py:518: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "test_data = pd.read_csv(\"../data/raw/test_values.csv\", index_col='row_id', \n",
    "    parse_dates=['timestamp'],)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. permanant changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "pid_array_test = test_data.process_id.values\n",
    "test_data.drop(['process_id'], axis=1, inplace=True)\n",
    "test_data.phase = test_data.phase.astype(phase_categorical)\n",
    "test_data.phase, _ = test_data.phase.factorize()\n",
    "test_data.phase = test_data.phase + 1\n",
    "test_data.timestamp = test_data.timestamp.view(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Transform using the preprocessor fit on our training data.\n",
    "\n",
    "Didn't work because our label encoder couldn't recognize those categories. (probably for new objects..)\n",
    "\n",
    "I don't think we'll have the same problem if we fit on the entire data, because then we'd know most objects. \n",
    "\n",
    "But then again, just in case test set has unseen objects, perhaps we're better off dropping `object_id` altogether?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "y contains new labels: [102 103 107 108 109 205 210 211 212 213 215 300 302 303 305 308 409 424\n 426 427 428 429 434 435 436 437 438 912 917 919 938 940 941 942 943 944\n 945 946 951 952 956 957 958 960 965]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-3e46931352d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpreprocessed_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda2/envs/biscuit-test/lib/python3.5/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m                 \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mXt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-04907d65c0b0>\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, dataframe)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcategorical_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             encoded = self.feature_encoder[feature].transform(\n\u001b[0;32m---> 31\u001b[0;31m                 \u001b[0mdataframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             )\n\u001b[1;32m     33\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/biscuit-test/lib/python3.5/site-packages/sklearn/preprocessing/label.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersect1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0mdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdiff1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"y contains new labels: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearchsorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: y contains new labels: [102 103 107 108 109 205 210 211 212 213 215 300 302 303 305 308 409 424\n 426 427 428 429 434 435 436 437 438 912 917 919 938 940 941 942 943 944\n 945 946 951 952 956 957 958 960 965]"
     ]
    }
   ],
   "source": [
    "preprocessed_test = preprocessor.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Reshape so that we have $(\\text{processes}, \\text{max length}, \\text{features})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_arr = np.zeros((len(pids), max_length, 84))\n",
    "for i, pid in enumerate(pids):\n",
    "    pid_mask = process_id_array == pid  # mask for this process id\n",
    "    pid_data = test_data[pid_mask]\n",
    "    preprocessed_data = preprocessor.transform(pid_data)\n",
    "    \n",
    "    nrows = min(max_length, preprocessed_data.shape[0]) \n",
    "    # In case some test processes have greater length..\n",
    "    # nip them at training max_length\n",
    "    test_arr[i, :nrows, :] = preprocessed_data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:biscuit-test]",
   "language": "python",
   "name": "conda-env-biscuit-test-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 464.4,
   "position": {
    "height": "40px",
    "left": "857.6px",
    "right": "20px",
    "top": "120px",
    "width": "344px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
