{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNNConfig(object):\n",
    "    def __init__(\n",
    "        cell_type='lstm', window=20, forget_bias=1.0, \n",
    "        n_hidden_cells=(100), keep_prob=1.0, batch_size=64, epoch_num=100,\n",
    "        learning_rate=0.01, max_grad_norm=1.0, init_scale=0.1,\n",
    "    ):\n",
    "        self.cell_type = cell_type\n",
    "        self.window = window\n",
    "        self.forget_bias = forget_bias\n",
    "        self.n_hidden_cells = n_hidden_cells\n",
    "        self.keep_prob = keep_prob\n",
    "        self.batch_size = batch_size\n",
    "        self.epoch_num = epoch_num\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.init_scale = init_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission_format.csv  test_values.csv\ttrain_labels.csv  train_values.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls 'Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\n",
    "    \"Data/train_values.csv\", \n",
    "    index_col='row_id', \n",
    "    parse_dates=['timestamp'], \n",
    "    nrows=100000  \n",
    "    # We'll just load the first 100K so we know\n",
    "    # how to load the data using code\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['process_id', 'object_id', 'phase', 'timestamp', 'pipeline',\n",
       "       'supply_flow', 'supply_pressure', 'return_temperature',\n",
       "       'return_conductivity', 'return_turbidity', 'return_flow', 'supply_pump',\n",
       "       'supply_pre_rinse', 'supply_caustic', 'return_caustic', 'supply_acid',\n",
       "       'return_acid', 'supply_clean_water', 'return_recovery_water',\n",
       "       'return_drain', 'object_low_level', 'tank_level_pre_rinse',\n",
       "       'tank_level_caustic', 'tank_level_acid', 'tank_level_clean_water',\n",
       "       'tank_temperature_pre_rinse', 'tank_temperature_caustic',\n",
       "       'tank_temperature_acid', 'tank_concentration_caustic',\n",
       "       'tank_concentration_acid', 'tank_lsh_caustic', 'tank_lsh_acid',\n",
       "       'tank_lsh_clean_water', 'tank_lsh_pre_rinse', 'target_time_period'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data PreProcessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Let's figure out how to preprocess the data into RNN-feedable form.\n",
    "\n",
    "    Let's separate out process_id. We don't want to encode or scale this information. We'll just reattach after encoding and scaling the rest.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_id_array = data.process_id.values\n",
    "data.drop(['process_id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "    phase will NOT be one-hot-encoded since there is an order to it, so we can simply encode it using 1, 2, .., 5.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase_categorical = CategoricalDtype(\n",
    "    categories=['pre_rinse', 'caustic', 'intermediate_rinse', \n",
    "                'acid', 'final_rinse'],\n",
    "    ordered=True\n",
    ")\n",
    "data.phase = data.phase.astype(phase_categorical)\n",
    "\n",
    "data.phase, phase_mapping_idx = data.phase.factorize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "row_id\n",
       "0        NaN\n",
       "1        NaN\n",
       "2        NaN\n",
       "3        NaN\n",
       "4        NaN\n",
       "5        NaN\n",
       "6        NaN\n",
       "7        NaN\n",
       "8        NaN\n",
       "9        NaN\n",
       "10       NaN\n",
       "11       NaN\n",
       "12       NaN\n",
       "13       NaN\n",
       "14       NaN\n",
       "15       NaN\n",
       "16       NaN\n",
       "17       NaN\n",
       "18       NaN\n",
       "19       NaN\n",
       "20       NaN\n",
       "21       NaN\n",
       "22       NaN\n",
       "23       NaN\n",
       "24       NaN\n",
       "25       NaN\n",
       "26       NaN\n",
       "27       NaN\n",
       "28       NaN\n",
       "29       NaN\n",
       "        ... \n",
       "99970    NaN\n",
       "99971    NaN\n",
       "99972    NaN\n",
       "99973    NaN\n",
       "99974    NaN\n",
       "99975    NaN\n",
       "99976    NaN\n",
       "99977    NaN\n",
       "99978    NaN\n",
       "99979    NaN\n",
       "99980    NaN\n",
       "99981    NaN\n",
       "99982    NaN\n",
       "99983    NaN\n",
       "99984    NaN\n",
       "99985    NaN\n",
       "99986    NaN\n",
       "99987    NaN\n",
       "99988    NaN\n",
       "99989    NaN\n",
       "99990    NaN\n",
       "99991    NaN\n",
       "99992    NaN\n",
       "99993    NaN\n",
       "99994    NaN\n",
       "99995    NaN\n",
       "99996    NaN\n",
       "99997    NaN\n",
       "99998    NaN\n",
       "99999    NaN\n",
       "Name: phase, Length: 100000, dtype: category\n",
       "Categories (5, object): [pre_rinse < caustic < intermediate_rinse < acid < final_rinse]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.phase.astype(phase_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['pre_rinse', 'caustic', 'intermediate_rinse', 'acid', 'final_rinse'], dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phase_mapping_idx.categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure each sequence is in the right order using the timestamp column, but once sequences are set up, discard the column.  \n",
    "-The column is only useful insofar as it tells us which data point comes before or after others.  \n",
    "-But then again, perhaps there is some signal from absoluate passage of time, so we can consider encoding this into UNIX times. Something to try out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.timestamp = data.timestamp.view(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Ensure each sequence is from single process using process_id, but don't include it in data since it really is just an ID column.\n",
    "\n",
    "    One-hot encode object_id and pipeline.\n",
    "        Although object_id is an ID column, there aren't that many objects (~100? NOT SURE) and there are multiple processes over each object. So object_id may carry valuable information.\n",
    "        I'm not sure what pipeline is, but we'll treat it as a categorical column\n",
    "        Below we first encode categorical columns into integers (and save that mapping in DataframeLabelEncoder.feature_encoder dictionary) and one-hot-encode them. (sklearn's OneHotEncoder requires that input is alreay integer-encoded.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['object_id', 'pipeline',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataframeLabelEncoder(object):\n",
    "    def __init__(self, categorical_features):\n",
    "        assert isinstance(categorical_features, (list, np.ndarray))\n",
    "        self.categorical_features = categorical_features\n",
    "        self.feature_encoder = dict()\n",
    "        self.cat_feature_mask = None\n",
    "        \n",
    "    def fit(self, dataframe):\n",
    "        assert isinstance(dataframe, pd.DataFrame)\n",
    "        \n",
    "        if self.cat_feature_mask is None:\n",
    "            self.cat_feature_mask = np.zeros(shape=dataframe.shape[1], dtype=bool)\n",
    "        \n",
    "        for i, feature in enumerate(categorical_features):\n",
    "            le = LabelEncoder()\n",
    "            le.fit(dataframe[feature])\n",
    "            self.feature_encoder[feature] = le\n",
    "            \n",
    "            index = np.where(dataframe.columns == feature)[0][0]\n",
    "            self.cat_feature_mask[index] = True\n",
    "        \n",
    "    def transform(self, dataframe):\n",
    "        assert dataframe.shape[1] == self.cat_feature_mask.shape[-1]\n",
    "        \n",
    "        array = dataframe.values.copy()\n",
    "        for feature in categorical_features:\n",
    "            encoded = self.feature_encoder[feature].transform(\n",
    "                dataframe[feature]\n",
    "            )\n",
    "            index = np.where(dataframe.columns == feature)[0][0]\n",
    "            array[:, index] = encoded\n",
    "            \n",
    "        return array\n",
    "    \n",
    "    def fit_transform(self, dataframe):\n",
    "        self.fit(dataframe)\n",
    "        return self.transform(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = DataframeLabelEncoder(categorical_features)\n",
    "label_encoder.fit(data)\n",
    "encoded_data = label_encoder.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 34)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create One-hot encoder\n",
    "ohe = OneHotEncoder(categorical_features=label_encoder.cat_feature_mask, sparse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data = ohe.fit_transform(encoded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 84)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is the expected shape: # non categorical features = 34-2 = 32, + # unique objects = 43, + # unique pipelines = 9. Thats 84."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From EDA, it looks like some are normally distributed, some exponential... we'll just standardize.\n",
    "\n",
    "I created the following thinking that we should only standardize columns that were originally numerical (and not boolean or categorical), but it's too much work.. I think just standardizing everything is fine!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumericColumnsStandardScaler(object):\n",
    "    def __init__(self, ohe_feature_indices):\n",
    "        assert isinstance(ohe_feature_indices, (list, np.ndarray))\n",
    "        self._ohe_feature_indices = ohe_feature_indices\n",
    "        self._scaler = StandardScaler()\n",
    "        self.numerical_mask = None\n",
    "        \n",
    "    def fit(self, data):\n",
    "        assert isinstance(data, np.array)\n",
    "        assert data.shape[1] == self.num_feature_mask.shape[-1]\n",
    "        \n",
    "        # we only want features that were numerical from the beginning\n",
    "        # So we create a mask that excludes all columns that are results of\n",
    "        # one-hot-encoding categorical columns.\n",
    "        mask = np.ones(data.shape[1], type=bool)\n",
    "        mask[:self._ohe_feature_indices[-1]] = 0\n",
    "        self.numerical_mask = mask\n",
    "        \n",
    "        self._scaler.fit(data[self.numerical_mask])\n",
    "        \n",
    "    def transform(self, data):\n",
    "        assert isinstance(data, np.array)\n",
    "        assert data.shape[1] == self.num_feature_mask.shape[-1]\n",
    "        scaled_num_data = self._scaler.transform(data[self.numerical_mask])\n",
    "        res = np.concatenate((data[~self.numerical_mask], scaled_num_data), axis=1)\n",
    "        print(res.shape)\n",
    "        return res\n",
    "    \n",
    "    def fit_transform(self, data):\n",
    "        self.fit(data)\n",
    "        return self.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv(\"Data/train_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Pipeline(\n",
    "    steps=[\n",
    "        ('label_encoder', label_encoder),\n",
    "        ('one_hot_encoder', ohe),\n",
    "        ('scaler', StandardScaler().fit(encoded_data))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "NOW THAT's OUR PREPROCESSOR!!! HOORAY!\n",
    "\n",
    "Ok let's just create the padded array now. (What Holden did!)\n",
    "\n",
    "From our EDA, we know the maximum length of a process is 15107. We also know there are 34 columns (excluding process id), but that gets encoded to 84 columns. Following Holdens' work,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "    (m, T, f) - where m is the number of unique processes, T is the number of time-sequences, and f is the number of features\n",
    "\n",
    "Our final array shall have shape $(82, 15107, 84)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(process_id_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.zeros((82, 15107, 84))\n",
    "for i, pid in enumerate(np.unique(process_id_array)):\n",
    "    pid_mask = process_id_array == pid  # mask for this process id\n",
    "    preprocessed_data = preprocessor.transform(data[pid_mask])\n",
    "    \n",
    "    nrows = pid_mask.sum()\n",
    "    arr[i, :nrows, :] = preprocessed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(process_id_array == pid).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "782"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pid_mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82, 15107, 84)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.19735816, -0.16218562, -0.14506451, ...,  0.        ,\n",
       "         0.        , -0.36748879],\n",
       "       [-0.19735816, -0.16218562, -0.14506451, ...,  0.        ,\n",
       "         0.        , -0.36748879],\n",
       "       [-0.19735816, -0.16218562, -0.14506451, ...,  0.        ,\n",
       "         0.        , -0.36748879],\n",
       "       ...,\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr[81]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_labels = labels[0:arr.shape[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82, 2)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:3697: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    }
   ],
   "source": [
    "subset_labels.drop(['process_id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>final_rinse_total_turbidity_liter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.318275e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.375286e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.271977e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.197830e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.133107e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.100847e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.058669e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.706871e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5.217553e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.037243e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.366682e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5.340851e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.875568e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3.279666e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.088902e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>6.682032e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4.339452e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3.148853e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.763055e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>6.893560e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.585300e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3.495283e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>9.830827e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.079254e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3.263581e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>6.604669e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5.537403e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5.900667e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.707947e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>7.588686e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>1.211227e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>4.811900e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>2.098077e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>1.715213e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>6.065643e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>1.707295e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>6.087690e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>9.774572e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>2.440315e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>8.323039e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>2.158018e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>7.909508e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>7.157491e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>8.758623e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>3.302757e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>3.404098e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>9.810773e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>4.325152e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>6.759069e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>6.104100e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>5.075102e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>5.527054e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>5.140244e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>1.154676e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>8.562062e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>5.839898e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>4.476624e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>5.449234e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>1.701053e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>7.151736e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>82 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    final_rinse_total_turbidity_liter\n",
       "0                        4.318275e+06\n",
       "1                        4.375286e+05\n",
       "2                        4.271977e+05\n",
       "3                        7.197830e+05\n",
       "4                        4.133107e+05\n",
       "5                        4.100847e+06\n",
       "6                        3.058669e+06\n",
       "7                        2.706871e+06\n",
       "8                        5.217553e+05\n",
       "9                        1.037243e+05\n",
       "10                       1.366682e+06\n",
       "11                       5.340851e+04\n",
       "12                       1.875568e+06\n",
       "13                       3.279666e+05\n",
       "14                       5.088902e+05\n",
       "15                       6.682032e+05\n",
       "16                       4.339452e+05\n",
       "17                       3.148853e+05\n",
       "18                       1.763055e+06\n",
       "19                       6.893560e+05\n",
       "20                       1.585300e+06\n",
       "21                       3.495283e+05\n",
       "22                       9.830827e+05\n",
       "23                       1.079254e+06\n",
       "24                       3.263581e+05\n",
       "25                       6.604669e+05\n",
       "26                       5.537403e+05\n",
       "27                       5.900667e+04\n",
       "28                       1.707947e+06\n",
       "29                       7.588686e+05\n",
       "..                                ...\n",
       "52                       1.211227e+06\n",
       "53                       4.811900e+05\n",
       "54                       2.098077e+05\n",
       "55                       1.715213e+06\n",
       "56                       6.065643e+05\n",
       "57                       1.707295e+05\n",
       "58                       6.087690e+05\n",
       "59                       9.774572e+06\n",
       "60                       2.440315e+05\n",
       "61                       8.323039e+04\n",
       "62                       2.158018e+05\n",
       "63                       7.909508e+05\n",
       "64                       7.157491e+05\n",
       "65                       8.758623e+04\n",
       "66                       3.302757e+07\n",
       "67                       3.404098e+05\n",
       "68                       9.810773e+05\n",
       "69                       4.325152e+05\n",
       "70                       6.759069e+05\n",
       "71                       6.104100e+05\n",
       "72                       5.075102e+05\n",
       "73                       5.527054e+05\n",
       "74                       5.140244e+05\n",
       "75                       1.154676e+05\n",
       "76                       8.562062e+04\n",
       "77                       5.839898e+05\n",
       "78                       4.476624e+05\n",
       "79                       5.449234e+05\n",
       "80                       1.701053e+05\n",
       "81                       7.151736e+06\n",
       "\n",
       "[82 rows x 1 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(arr, subset_labels, test_size=0.1, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(50, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mae', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 73 samples, validate on 9 samples\n",
      "Epoch 1/50\n",
      " - 97s - loss: 1433786.2962 - val_loss: 2326203.0000\n",
      "Epoch 2/50\n",
      " - 92s - loss: 1433786.2962 - val_loss: 2326203.0000\n",
      "Epoch 3/50\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test), verbose=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
